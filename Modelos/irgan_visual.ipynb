{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"irgan_visual.ipynb","provenance":[],"collapsed_sections":["oz0bbxx-tsKB"],"machine_shape":"hm"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.10"},"toc":{"base_numbering":1,"nav_menu":{},"number_sections":true,"sideBar":true,"skip_h1_title":false,"title_cell":"Table of Contents","title_sidebar":"Contents","toc_cell":false,"toc_position":{"height":"calc(100% - 180px)","left":"10px","top":"150px","width":"296.962px"},"toc_section_display":true,"toc_window_display":true},"varInspector":{"cols":{"lenName":16,"lenType":16,"lenVar":40},"kernels_config":{"python":{"delete_cmd_postfix":"","delete_cmd_prefix":"del ","library":"var_list.py","varRefreshCmd":"print(var_dic_list())"},"r":{"delete_cmd_postfix":") ","delete_cmd_prefix":"rm(","library":"var_list.r","varRefreshCmd":"cat(var_dic_list()) "}},"types_to_exclude":["module","function","builtin_function_or_method","instance","_Feature"],"window_display":false}},"cells":[{"cell_type":"markdown","metadata":{"id":"e9ysjia76BOp"},"source":["# IRGAN"]},{"cell_type":"markdown","metadata":{"id":"JeR_hmKqpnAB"},"source":["Codigo:\n","\n","https://github.com/geek-ai/irgan\n","\n","paper: \n","\n","https://arxiv.org/pdf/1705.10513.pdf"]},{"cell_type":"markdown","metadata":{"id":"7T9tLDtVBlgX"},"source":["Una GAN discreta (como SeqGan) pero que el generador es modelado como un politica de aprendizaje reforzado para seleccionar el documento candidato $d$ en el estado dado la query $q_n$, y una metrica de relevancia $r$ que es entrenado via una politica de gradientes"]},{"cell_type":"markdown","metadata":{"id":"oREzkvuF6DhH"},"source":["Quieren poner enfoque en el entrenamiento adversario, osea usan modelos simples e iguales para D y G"]},{"cell_type":"markdown","metadata":{"id":"18oHp-BWc4Gw"},"source":["El módulo pickle implementa un algoritmo para convertir un objeto Python arbitrario en una serie de bytes. Este proceso también se llama serializar \"el objeto\". El flujo de bytes que representa el objeto se puede transmitir o almacenar y luego reconstruir para crear un nuevo objeto con las mismas características.\n","\n","El módulo cPickle implementa el mismo algoritmo, en C en lugar de Python. Es muchas veces más rápido que la implementación de Python, pero no permite al usuario crear una subclase de Pickle. Si la subclasificación no es importante para su uso, probablemente desee utilizar cPickle."]},{"cell_type":"code","metadata":{"id":"WZmLNRllc5yX"},"source":["import _pickle as cPickle #guarda el modelo..\n","from IPython.display import Latex\n","import tensorflow as tf\n","import numpy as np\n","import multiprocessing\n","import pandas as pd \n","import random\n","import pickle\n","\n","cores = multiprocessing.cpu_count()\n","random.seed(0)\n","\n","\n","import matplotlib.pyplot as plt\n","import heapq\n","tf.compat.v1.disable_eager_execution()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Wa37Wu_tmZ0Q"},"source":["## Discriminativo DNS"]},{"cell_type":"code","metadata":{"id":"xkx5B9xJciJ6"},"source":["\n","class DIS():\n","    def __init__(self, itemNum, userNum, emb_dim,visual_matrix, visual_emb, lamda, param=None, initdelta=0.05, learning_rate=0.05,imageFeatureDim= 4096):\n","        self.itemNum = itemNum\n","        self.userNum = userNum\n","        self.emb_dim = emb_dim\n","        self.lamda = lamda  # regularization parameters\n","        self.param = param\n","        self.visual_I = visual_matrix #preentrenada no aprendible\n","        self.k2 = visual_emb\n","        self.initdelta = initdelta\n","        self.learning_rate = learning_rate\n","        self.imageFeatureDim= imageFeatureDim\n","        self.d_params = []\n","\n","        with tf.compat.v1.compat.v1.variable_scope('discriminator'):\n","            if self.param is None: #Si no hay param -> inicializacion aleatoria\n","                self.user_embeddings = tf.compat.v1.Variable(\n","                    tf.compat.v1.random_uniform([self.userNum, self.emb_dim], minval=-self.initdelta, maxval=self.initdelta,\n","                                      dtype=tf.compat.v1.float32))\n","                self.item_embeddings = tf.compat.v1.Variable(\n","                    tf.compat.v1.random_uniform([self.itemNum, self.emb_dim], minval=-self.initdelta, maxval=self.initdelta,\n","                                      dtype=tf.compat.v1.float32))\n","                self.item_bias = tf.compat.v1.Variable(tf.compat.v1.zeros([self.itemNum]))\n","                #####################################\n","                self.visual_U = tf.compat.v1.Variable(\n","                    tf.compat.v1.random_uniform([self.userNum, self.k2], minval=-self.initdelta, maxval=self.initdelta,\n","                                      dtype=tf.compat.v1.float32))\n","                #E\n","                self.itemEmb_W = tf.compat.v1.Variable(\n","                    tf.compat.v1.random_uniform([self.imageFeatureDim,self.k2], minval=-self.initdelta, maxval=self.initdelta,\n","                                      dtype=tf.compat.v1.float32))\n","                #beta\n","                self.visual_item_bias = tf.compat.v1.Variable(\n","                    tf.compat.v1.random_uniform([self.itemNum,self.imageFeatureDim], minval=-self.initdelta, maxval=self.initdelta,\n","                                      dtype=tf.compat.v1.float32))\n","\n","                ###################################################\n","            else: #lee los parametros del archivo\n","                self.user_embeddings = tf.compat.v1.Variable(self.param[0])\n","                self.item_embeddings = tf.compat.v1.Variable(self.param[1])\n","                self.item_bias = tf.compat.v1.Variable(self.param[2])\n","                #######################################\n","                self.visual_U = tf.compat.v1.Variable(self.param[3])\n","                self.itemEmb_W = tf.compat.v1.Variable(self.param[4])\n","                self.visual_item_bias = tf.compat.v1.Variable(self.param[5])\n","                ###########################\n","           \n","            self.d_params = [self.user_embeddings, self.item_embeddings, self.item_bias,  self.visual_U, self.itemEmb_W, self.visual_item_bias ]\n","        \n","        # placeholder definition\n","        self.u = tf.compat.v1.placeholder(tf.compat.v1.int32)\n","        self.pos = tf.compat.v1.placeholder(tf.compat.v1.int32)\n","        self.neg = tf.compat.v1.placeholder(tf.compat.v1.int32)\n","\n","        self.u_embedding = tf.compat.v1.nn.embedding_lookup(self.user_embeddings, self.u)\n","        self.pos_embedding = tf.compat.v1.nn.embedding_lookup(self.item_embeddings, self.pos)\n","        self.pos_bias = tf.compat.v1.gather(self.item_bias, self.pos)\n","        self.neg_embedding = tf.compat.v1.nn.embedding_lookup(self.item_embeddings, self.neg)\n","        self.neg_bias = tf.compat.v1.gather(self.item_bias, self.neg)\n","\n","\n","        ##################################################################################\n","\n","        self.visual_U_vector = tf.compat.v1.nn.embedding_lookup(self.visual_U, self.u)\n","        self.visual_I_matrix_pos = tf.compat.v1.nn.embedding_lookup(self.visual_I, self.pos) #[1,4096]\n","        self.visual_I_matrix_neg = tf.compat.v1.nn.embedding_lookup(self.visual_I, self.neg)\n","        self.visual_pos_bias = tf.compat.v1.gather(self.visual_item_bias, self.pos) #[1,4096]\n","        self.visual_neg_bias = tf.compat.v1.gather(self.visual_item_bias, self.neg)\n","\n","       ##################\n","\n","        self.Ef_pos= tf.matmul( self.visual_I_matrix_pos,self.itemEmb_W) #f_i x E= [1,4096]x[4096,k2]= [1,k2]\n","        self.Ef_neg= tf.matmul(self.visual_I_matrix_neg,self.itemEmb_W) \n","        self.Ef= tf.compat.v1.matmul(self.itemEmb_W,self.visual_I,transpose_a=True,transpose_b=True ) #E^T x f^T =  [k2,4096]x[4096,#I] = [k2,#I]\n","        self.bf_pos=tf.compat.v1.reduce_sum(tf.compat.v1.multiply(self.visual_I_matrix_pos, self.visual_pos_bias),1)#[1,4096]x[1,4096]\n","        self.bf_neg=  tf.compat.v1.reduce_sum(tf.compat.v1.multiply(self.visual_I_matrix_neg, self.visual_neg_bias),1)\n","          \n","        \n","        ##################\n","        \n","        self.pre_logits = tf.compat.v1.sigmoid(\n","            tf.compat.v1.reduce_sum(tf.compat.v1.multiply(self.u_embedding, self.pos_embedding - self.neg_embedding),1) # [1,k]x[1,k] multuply es element wise, reduce sum sum, osea producto punto.\n","             + self.pos_bias - self.neg_bias\n","             +  tf.compat.v1.reduce_sum( tf.compat.v1.multiply(self.visual_U_vector,self.Ef_pos- self.Ef_neg ) ,1 )  # [1,k2]x[1,k2]\n","             +self.bf_pos- self.bf_neg \n","                          ) \n","             \n","        \n"," \n","        self.reg= self.lamda * (\n","            tf.compat.v1.nn.l2_loss(self.u_embedding) +\n","            tf.compat.v1.nn.l2_loss(self.pos_embedding) +\n","            tf.compat.v1.nn.l2_loss(self.pos_bias) +\n","            tf.compat.v1.nn.l2_loss(self.neg_embedding) +\n","            tf.compat.v1.nn.l2_loss(self.neg_bias)+\n","            tf.compat.v1.nn.l2_loss(self.visual_U_vector) +\n","            tf.compat.v1.nn.l2_loss(self.itemEmb_W) +\n","            tf.compat.v1.nn.l2_loss(self.visual_neg_bias) +\n","            tf.compat.v1.nn.l2_loss(self.visual_pos_bias)\n","            )\n","\n","        self.pre_loss = -tf.compat.v1.reduce_mean(tf.compat.v1.log(self.pre_logits)) + self.reg\n","        d_opt = tf.compat.v1.train.GradientDescentOptimizer(self.learning_rate)\n","        self.d_updates = d_opt.minimize(self.pre_loss, var_list=self.d_params)\n","\n","        # for test stage, self.u: [batch_size]\n","        \n","                          # uxI= [batch_size,k]x[k,#I]=[batch_size,#I]                                                     # b_i= [1, #I]        # [batch_size, k2]x[k2, #I]  = [bact_size,#I]                                             #beta elementwise f = [#,4096]elementwise[#I,4096]= [1, #I]\n","        self.all_rating = tf.compat.v1.matmul(self.u_embedding, self.item_embeddings, transpose_a=False,transpose_b=True) + self.item_bias + tf.compat.v1.matmul(self.visual_U_vector, self.Ef, transpose_a=False, transpose_b=False) +  tf.compat.v1.reduce_sum(tf.compat.v1.multiply(self.visual_item_bias , self.visual_I),1)                    \n","\n","        #self.all_logits = tf.compat.v1.reduce_sum(tf.compat.v1.multiply(self.u_embedding, self.item_embeddings), 1) + self.item_bias\n","        # for dns sample\n","        #esto da muy grande\n","                                                                        # [1,k]x[#I,k] = [1,#I]                                 #[1, #I]                                                      #[1,k2]x[#I,k2]=[1,#I]\n","        self.dns_rating = tf.compat.v1.reduce_sum(tf.compat.v1.multiply(self.u_embedding, self.item_embeddings), 1) + self.item_bias + tf.compat.v1.reduce_sum(tf.compat.v1.multiply(self.visual_U_vector, tf.transpose(self.Ef) ), 1) + tf.compat.v1.reduce_sum(tf.compat.v1.multiply(self.visual_item_bias , self.visual_I),1)  \n","\n","    def save_model(self, sess, filename):\n","        param = sess.run(self.d_params)\n","        cPickle.dump(param, open(filename, 'wb'))  #antes w"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oz0bbxx-tsKB"},"source":["## utils.py"]},{"cell_type":"code","metadata":{"id":"xfkSKD5CtwxK"},"source":["import linecache\n","import numpy as np\n","\n","\n","def file_len(fname):\n","    with open(fname) as f:\n","        for i, l in enumerate(f):\n","            pass\n","    return i + 1\n","\n","\n","# Get batch data from training set\n","def get_batch_data(file, index, size):  # 1,5->1,2,3,4,5\n","    user = []\n","    item = []\n","    label = []\n","    for i in range(index, index + size):\n","        line = linecache.getline(file, i)\n","        line = line.strip()\n","        line = line.split()\n","        user.append(int(line[0]))\n","        user.append(int(line[0]))\n","        item.append(int(line[1]))\n","        item.append(int(line[2]))\n","        label.append(1.)\n","        label.append(0.)\n","    return user, item, label\n","\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PjnuefL1mdUU"},"source":["## Generador"]},{"cell_type":"code","metadata":{"id":"jEM2J1CTmhOn"},"source":["class GEN():\n","    def __init__(self, itemNum, userNum, emb_dim, visual_matrix, visual_emb, lamda, param=None, initdelta=0.05, learning_rate=0.05,imageFeatureDim= 4096):\n","        self.itemNum = itemNum\n","        self.userNum = userNum\n","        self.emb_dim = emb_dim\n","        self.lamda = lamda  # regularization parameters\n","        self.param = param\n","        self.initdelta = initdelta\n","        self.learning_rate = learning_rate\n","        self.g_params = []\n","        self.visual_I = visual_matrix #preentrenada no aprendible\n","        self.k2 = visual_emb\n","        self.imageFeatureDim= imageFeatureDim\n","\n","        with tf.compat.v1.variable_scope('generator'):\n","            if self.param == None: #inicialización de representaciones\n","                self.user_embeddings = tf.compat.v1.Variable(\n","                    tf.compat.v1.random_uniform([self.userNum, self.emb_dim], minval=-self.initdelta, maxval=self.initdelta,\n","                                      dtype=tf.compat.v1.float32))\n","                self.item_embeddings = tf.compat.v1.Variable(\n","                    tf.compat.v1.random_uniform([self.itemNum, self.emb_dim], minval=-self.initdelta, maxval=self.initdelta,\n","                                      dtype=tf.compat.v1.float32))\n","                self.item_bias = tf.compat.v1.Variable(tf.compat.v1.zeros([self.itemNum]))\n","                #####################################\n","                self.visual_U = tf.compat.v1.Variable(\n","                    tf.compat.v1.random_uniform([self.userNum, self.k2], minval=-self.initdelta, maxval=self.initdelta,\n","                                      dtype=tf.compat.v1.float32))\n","                #E\n","                self.itemEmb_W = tf.compat.v1.Variable(\n","                    tf.compat.v1.random_uniform([self.imageFeatureDim,self.k2], minval=-self.initdelta, maxval=self.initdelta,\n","                                      dtype=tf.compat.v1.float32))\n","                #beta\n","                self.visual_item_bias = tf.compat.v1.Variable(\n","                    tf.compat.v1.random_uniform([self.itemNum,self.imageFeatureDim], minval=-self.initdelta, maxval=self.initdelta,\n","                                      dtype=tf.compat.v1.float32))\n","\n","                ###################################################\n","\n","\n","            else:\n","                self.user_embeddings = tf.compat.v1.Variable(self.param[0])\n","                self.item_embeddings = tf.compat.v1.Variable(self.param[1])\n","                self.item_bias = tf.compat.v1.Variable(param[2])\n","                                #######################################\n","                self.visual_U = tf.compat.v1.Variable(self.param[3])\n","                self.itemEmb_W = tf.compat.v1.Variable(self.param[4])\n","                self.visual_item_bias = tf.compat.v1.Variable(self.param[5])\n","                ###########################\n","\n","            self.g_params = [self.user_embeddings, self.item_embeddings, self.item_bias,self.visual_U, self.itemEmb_W, self.visual_item_bias ]\n","\n","        self.u = tf.compat.v1.placeholder(tf.compat.v1.int32)\n","        self.i = tf.compat.v1.placeholder(tf.compat.v1.int32)\n","        self.reward = tf.compat.v1.placeholder(tf.compat.v1.float32)\n","\n","        self.u_embedding = tf.compat.v1.nn.embedding_lookup(self.user_embeddings, self.u)\n","        self.i_embedding = tf.compat.v1.nn.embedding_lookup(self.item_embeddings, self.i)\n","        self.i_bias = tf.compat.v1.gather(self.item_bias, self.i)\n","\n","\n","        ##################################################################################\n","\n","        self.visual_U_vector = tf.compat.v1.nn.embedding_lookup(self.visual_U, self.u)\n","        self.visual_I_matrix_i= tf.compat.v1.nn.embedding_lookup(self.visual_I, self.i) #[1,4096]\n","        self.visual_bias_i = tf.compat.v1.gather(self.visual_item_bias, self.i) #[1,4096]\n","\n","       ##################\n","\n","        self.Ef_i= tf.matmul( self.visual_I_matrix_i,self.itemEmb_W) #f_i x E= [1,4096]x[4096,k2]= [1,k2]\n","        self.Ef= tf.compat.v1.matmul(self.itemEmb_W,self.visual_I,transpose_a=True,transpose_b=True ) #E^T x f^T =  [k2,4096]x[4096,#I] = [k2,#I]\n","        self.bf_i=tf.compat.v1.reduce_sum(tf.compat.v1.multiply(self.visual_I_matrix_i, self.visual_bias_i),1)#[1,4096]x[1,4096]\n","          \n","        \n","        ##################\n","                                                                                                           \n","                                                                        #[1,k]x[#I,k]                                   #[1,#I]                                # [1,k2]x[1,k2]\n","        self.all_logits = tf.compat.v1.reduce_sum(tf.compat.v1.multiply(self.u_embedding, self.item_embeddings), 1) + self.item_bias +  tf.compat.v1.reduce_sum(tf.compat.v1.multiply(self.visual_U_vector, tf.transpose(self.Ef) ), 1) + tf.compat.v1.reduce_sum(tf.compat.v1.multiply(self.visual_item_bias , self.visual_I),1)  \n","\n","        self.i_prob = tf.compat.v1.gather(\n","            tf.compat.v1.reshape(tf.compat.v1.nn.softmax(tf.compat.v1.reshape(self.all_logits, [1, -1])), [-1]),\n","            self.i)\n","\n","        self.reg= self.lamda * (tf.compat.v1.nn.l2_loss(self.u_embedding) + tf.compat.v1.nn.l2_loss(self.i_embedding) + tf.compat.v1.nn.l2_loss(self.i_bias)+\n","                                 tf.compat.v1.nn.l2_loss(self.visual_U_vector) +tf.compat.v1.nn.l2_loss(self.itemEmb_W) +tf.compat.v1.nn.l2_loss(self.visual_bias_i) )\n","        \n","        self.gan_loss = -tf.compat.v1.reduce_mean(tf.compat.v1.log(self.i_prob) * self.reward) + self.reg\n","\n","        g_opt = tf.compat.v1.train.GradientDescentOptimizer(self.learning_rate)\n","\n","        self.gan_updates = g_opt.minimize(self.gan_loss, var_list=self.g_params)\n","\n","        # for test stage, self.u: [batch_size]\n","        self.all_rating = tf.compat.v1.matmul(self.u_embedding, self.item_embeddings, transpose_a=False,\n","                                    transpose_b=True) + self.item_bias + tf.compat.v1.matmul(self.visual_U_vector, self.Ef, transpose_a=False, transpose_b=False) +  tf.compat.v1.reduce_sum(tf.compat.v1.multiply(self.visual_item_bias , self.visual_I),1) \n","\n","    def save_model(self, sess, filename):\n","        param = sess.run(self.g_params)\n","        cPickle.dump(param, open(filename, 'wb')) #mismo cambio"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EzgYVrjhtGBa"},"source":["## Discriminador"]},{"cell_type":"markdown","metadata":{"id":"2Y1XtLZr2Lzr"},"source":["$f_{\\phi} (q,d)= s_{\\phi} (q,d)= s(u,i)= b_i + v_u^Tv_i$\n","\n","Máquina de factorización sencilla\n","\n","Su labor es distinguir (clasificar) si el documento $d$ es relevante (real, etiqueta=1, ejemplo positivo) para la consulta $q$. Estimando la probabilidad:\n","\n","$D(d|q)= \\sigma (f_{\\phi} (q,d))= \\dfrac{exp(f_{\\phi} (q,d))}{1+ exp(f_{\\phi} (q,d))} $\n","\n","Su objetivo entonces es maximiza la log verosimititud de distrinción entre documentos verdaderos y generados:\n","\n","$\\phi^{*}=\\arg \\max _{\\phi} \\sum_{n=1}^{N}\\left(\\mathbb{E}_{d \\sim p_{\\text {true }}\\left(d \\mid q_{n}, r\\right)}\\left[\\log \\left(\\sigma\\left(f_{\\phi}\\left(d, q_{n}\\right)\\right)\\right]+\\right.\\right.$\n","$\\left.\\mathbb{E}_{d \\sim p_{\\theta^{*}}\\left(d \\mid q_{n}, r\\right)}\\left[\\log \\left(1-\\sigma\\left(f_{\\phi}\\left(d, q_{n}\\right)\\right)\\right)\\right]\\right)$\n","\n","Modelo generativo actual fijo"]},{"cell_type":"code","metadata":{"id":"QhMzSMIttFGV"},"source":["class DIS():\n","    def __init__(self, itemNum, userNum, emb_dim, visual_matrix, visual_emb,lamda, param=None, initdelta=0.05, learning_rate=0.05,imageFeatureDim=4096):\n","        self.itemNum = itemNum\n","        self.userNum = userNum\n","        self.emb_dim = emb_dim\n","        self.lamda = lamda  # regularization parameters\n","        self.param = param\n","        self.initdelta = initdelta\n","        self.learning_rate = learning_rate\n","        self.d_params = []\n","        self.visual_I = visual_matrix #preentrenada no aprendible\n","        self.k2 = visual_emb\n","        self.imageFeatureDim= imageFeatureDim\n","\n","        with tf.compat.v1.variable_scope('discriminator'):\n","          #inicializacion\n","            if self.param == None:\n","                self.user_embeddings = tf.compat.v1.Variable(\n","                    tf.compat.v1.random_uniform([self.userNum, self.emb_dim], minval=-self.initdelta, maxval=self.initdelta,\n","                                      dtype=tf.compat.v1.float32))\n","                self.item_embeddings = tf.compat.v1.Variable(\n","                    tf.compat.v1.random_uniform([self.itemNum, self.emb_dim], minval=-self.initdelta, maxval=self.initdelta,\n","                                      dtype=tf.compat.v1.float32))\n","                self.item_bias = tf.compat.v1.Variable(tf.compat.v1.zeros([self.itemNum]))\n","                #####################################\n","                self.visual_U = tf.compat.v1.Variable(\n","                    tf.compat.v1.random_uniform([self.userNum, self.k2], minval=-self.initdelta, maxval=self.initdelta,\n","                                      dtype=tf.compat.v1.float32))\n","                #E\n","                self.itemEmb_W = tf.compat.v1.Variable(\n","                    tf.compat.v1.random_uniform([self.imageFeatureDim,self.k2], minval=-self.initdelta, maxval=self.initdelta,\n","                                      dtype=tf.compat.v1.float32))\n","                #beta\n","                self.visual_item_bias = tf.compat.v1.Variable(\n","                    tf.compat.v1.random_uniform([self.itemNum,self.imageFeatureDim], minval=-self.initdelta, maxval=self.initdelta,\n","                                      dtype=tf.compat.v1.float32))\n","\n","                ###################################################\n","\n","\n","            else:\n","                self.user_embeddings = tf.compat.v1.Variable(self.param[0])\n","                self.item_embeddings = tf.compat.v1.Variable(self.param[1])\n","                self.item_bias = tf.compat.v1.Variable(self.param[2])\n","                #######################################\n","                self.visual_U = tf.compat.v1.Variable(self.param[3])\n","                self.itemEmb_W = tf.compat.v1.Variable(self.param[4])\n","                self.visual_item_bias = tf.compat.v1.Variable(self.param[5])\n","                ###########################\n","\n","        self.d_params = [self.user_embeddings, self.item_embeddings, self.item_bias,self.visual_U, self.itemEmb_W, self.visual_item_bias ]\n","\n","        # placeholder definition\n","        self.u = tf.compat.v1.placeholder(tf.compat.v1.int32)\n","        self.i = tf.compat.v1.placeholder(tf.compat.v1.int32)\n","        self.label = tf.compat.v1.placeholder(tf.compat.v1.float32)\n","        #indexaciones\n","        #V_u\n","        self.u_embedding = tf.compat.v1.nn.embedding_lookup(self.user_embeddings, self.u)\n","        #V_i\n","        self.i_embedding = tf.compat.v1.nn.embedding_lookup(self.item_embeddings, self.i)\n","        #b_i\n","        self.i_bias = tf.compat.v1.gather(self.item_bias, self.i)\n","\n","\n","        ##################################################################################\n","\n","        self.visual_U_vector = tf.compat.v1.nn.embedding_lookup(self.visual_U, self.u)\n","        self.visual_I_matrix_i= tf.compat.v1.nn.embedding_lookup(self.visual_I, self.i) #[1,4096]\n","        self.visual_bias_i = tf.compat.v1.gather(self.visual_item_bias, self.i) #[1,4096]\n","\n","       ##################\n","        self.Ef_i= tf.matmul( self.visual_I_matrix_i,self.itemEmb_W) #f_i x E= [1,4096]x[4096,k2]= [1,k2]\n","        self.Ef= tf.compat.v1.matmul(self.itemEmb_W,self.visual_I,transpose_a=True,transpose_b=True ) #E^T x f^T =  [k2,4096]x[4096,#I] = [k2,#I]\n","        self.bf_i=tf.compat.v1.reduce_sum(tf.compat.v1.multiply(self.visual_I_matrix_i, self.visual_bias_i),1)#[1,4096]x[1,4096]\n","          \n","        \n","        ##################\n","\n","        #b_i+vu*v_i\n","        self.pre_logits = tf.compat.v1.reduce_sum(tf.compat.v1.multiply(self.u_embedding, self.i_embedding), 1) + self.i_bias + tf.compat.v1.reduce_sum( tf.compat.v1.multiply(self.visual_U_vector,self.Ef_i ) ,1 ) + self.bf_i\n","        #sigmoide cross entropy con regularización l2 \n","        self.reg= self.lamda * (\n","            tf.compat.v1.nn.l2_loss(self.u_embedding) + tf.compat.v1.nn.l2_loss(self.i_embedding) + tf.compat.v1.nn.l2_loss(self.i_bias)+\n","                                 tf.compat.v1.nn.l2_loss(self.visual_U_vector) +tf.compat.v1.nn.l2_loss(self.itemEmb_W) +tf.compat.v1.nn.l2_loss(self.visual_bias_i) )\n","        \n","        self.pre_loss = tf.compat.v1.nn.sigmoid_cross_entropy_with_logits(labels=self.label,\n","                                                                logits=self.pre_logits) + self.reg\n","        #SGD\n","        d_opt = tf.compat.v1.train.GradientDescentOptimizer(self.learning_rate)\n","        self.d_updates = d_opt.minimize(self.pre_loss, var_list=self.d_params)\n","        #Recompensa: \n","        #b_i+vu*v_i\n","        self.reward_logits = tf.compat.v1.reduce_sum(tf.compat.v1.multiply(self.u_embedding, self.i_embedding),\n","                                           1) + self.i_bias +  tf.compat.v1.reduce_sum( tf.compat.v1.multiply(self.visual_U_vector,self.Ef_i ) ,1 ) + self.bf_i\n","        #2sig(f)-1\n","        self.reward = 2 * (tf.compat.v1.sigmoid(self.reward_logits) - 0.5)\n","\n","        # for test stage, self.u: [batch_size]\n","        self.all_rating = tf.compat.v1.matmul(self.u_embedding, self.item_embeddings, transpose_a=False,\n","                                    transpose_b=True) + self.item_bias + tf.compat.v1.matmul(self.visual_U_vector, self.Ef, transpose_a=False, transpose_b=False) +  tf.compat.v1.reduce_sum(tf.compat.v1.multiply(self.visual_item_bias , self.visual_I),1) \n","\n","        #1 usuario todos los items.\n","        \n","        self.all_logits = tf.compat.v1.reduce_sum(tf.compat.v1.multiply(self.u_embedding, self.item_embeddings), 1) + self.item_bias +  tf.compat.v1.reduce_sum(tf.compat.v1.multiply(self.visual_U_vector, tf.transpose(self.Ef) ), 1) + tf.compat.v1.reduce_sum(tf.compat.v1.multiply(self.visual_item_bias , self.visual_I),1)  \n","        \n","        #self.NLL = -tf.compat.v1.reduce_mean(tf.compat.v1.log(\n","            #tf.compat.v1.gather(tf.compat.v1.reshape(tf.compat.v1.nn.softmax(tf.compat.v1.reshape(self.all_logits, [1, -1])), [-1]), self.i)))\n","        # for dns sample\n","         \n","        self.dns_rating = tf.compat.v1.reduce_sum(tf.compat.v1.multiply(self.u_embedding, self.item_embeddings), 1) + self.item_bias + tf.compat.v1.reduce_sum(tf.compat.v1.multiply(self.visual_U_vector, tf.transpose(self.Ef) ), 1) + tf.compat.v1.reduce_sum(tf.compat.v1.multiply(self.visual_item_bias , self.visual_I),1) \n","\n","    def save_model(self, sess, filename):\n","        param = sess.run(self.d_params)\n","        cPickle.dump(param, open(filename, 'w'))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iex1cUN48yNZ"},"source":["# Adaptación de IRGAN"]},{"cell_type":"markdown","metadata":{"id":"Rfo9HdFNyRa6"},"source":["## Lectura Amazon "]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"p-Rey0DTyRa6","outputId":"90fbdc03-f43d-4d43-9cc1-1b5ffe858e64"},"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n","#workdir = '/content/drive/MyDrive/dataset_ml/amazon_men_actualizado/' \n","workdir = '/content/drive/MyDrive/dataset_ml/women/' \n","\n","usuarios_train = np.load(workdir+'dic_train_women.npy',allow_pickle='TRUE')\n","usuarios_train=list(usuarios_train.reshape(-1,1))[0][0]\n","usuarios_test = np.load(workdir+'dic_test_women.npy',allow_pickle='TRUE')\n","usuarios_test=list(usuarios_test.reshape(-1,1))[0][0]\n","DIS_TRAIN_FILE =workdir+ \"dis-train.txt\"\n","DIS_MODEL_FILE =   workdir+\"model_dns_IRGANvisual.pkl\" #rutaaaa"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","metadata":{"id":"GfCSPCJ-yRa6"},"source":["dic_feautures = np.load(workdir+'features_women_effnet.npy',allow_pickle='TRUE')\n","dic_feautures=list(dic_feautures.reshape(-1,1))[0][0]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4wKFa7dUyRa7"},"source":["#########################################################################################\n","# Hyper-parameters\n","#########################################################################################\n","EMB_DIM = 10\n","K2=500\n","DNS_K = 5"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BHYsg7rQyRa7"},"source":["user_pos_test = {}\n","user_pos_train= {}\n","c_t=0\n","all_items= []\n","\n","\n","for u in usuarios_train.keys():\n","  for item in usuarios_train[u]:\n","    if item[b\"productid\"] not in all_items:\n","      all_items.append(item[b\"productid\"])\n","    \n","    if u not in user_pos_train: #nuevo\n","      user_pos_train[u] = [item[b\"productid\"]]\n","    else: #sino agrego\n","      user_pos_train[u].append(item[b\"productid\"])\n","all_user= list(user_pos_train.keys())\n","\n","for u in usuarios_train.keys():    \n","  item=usuarios_test[u][0]\n","  if item[b\"productid\"] not in all_items:\n","    c_t+=1\n","  else:\n","    user_pos_test[u] = [item[b\"productid\"]]\n","\n","ITEM_NUM=len(all_items)\n","USER_NUM= len(usuarios_train.keys())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1y3AlUDTyRa7","outputId":"488aa28f-a5b4-48be-ac2a-d0c647afa849"},"source":["print(\"% de items en test que no estan en train: \",c_t/len(all_items)*100)\n","print(\"n item: \", ITEM_NUM, \"n usuarios: \", USER_NUM)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["% de items en test que no estan en train:  2.7743271221532093\n","n item:  7245 n usuarios:  3158\n"]}]},{"cell_type":"code","metadata":{"id":"kOtSW4BvyRa7"},"source":["c=0\n","for i in all_items:\n","  vector= dic_feautures[i]\n","  if c==0:\n","    visual_matrix= vector\n","    c=1\n","  else:\n","    visual_matrix= np.concatenate((visual_matrix, vector),axis=0)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QkeYlTfc2DFO","outputId":"d0adfce8-427f-4d54-a236-e21b4781c88f"},"source":["print(visual_matrix.shape)"],"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["(9106, 1536)\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dPhzAUnEyRa7","outputId":"4203ab38-85f5-4798-d558-813443c29098"},"source":["np.max(visual_matrix)"],"execution_count":null,"outputs":[{"data":{"text/plain":["21.523172"]},"execution_count":22,"metadata":{},"output_type":"execute_result"}]},{"cell_type":"code","metadata":{"id":"SkUTjJ8uyRa7"},"source":["visual_matrix= visual_matrix/np.max(visual_matrix)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZBwKKVux0l4g"},"source":["## Inicialiazacion DNS"]},{"cell_type":"code","metadata":{"id":"IDimN-f3xfpd"},"source":["def generate_dns(sess, model_, filename): #dy\n","    \"\"\"\"\n","    crea tripletas u,i,j con los id originales\n","    \"\"\"\n","    data = []\n","    for u in list(user_pos_train.keys()): #para cada usuario\n","        pos = user_pos_train[u] #lo que consumio\n","        all_rating = sess.run(model_.dns_rating, {model_.u: all_user.index(u)}) #evalua el modelo\n","        all_rating = np.array(all_rating)\n","        neg = []\n","        candidates = list(set(all_items) - set(pos) )#los que no ha consumido\n","\n","        for _ in range(len(pos)): #para cada item\n","            choice = np.random.choice(candidates, DNS_K) #se escogen DN_K aleatoriamente de los candidatos\n","            choice= [list(all_items).index(i) for i in choice]\n","            choice_score = all_rating[choice] # sus rating\n","            neg.append(choice[np.argmax(choice_score)] ) #se escoge el items mas cercanos al gusto de esta muestra aleatoria \n","            #top_10_ind=random.sample(heapq.nlargest(10, range(len(choice_score)), choice_score.take),1)\n","            #neg.append(choice[top_10_ind[0]])\n","        for i in range(len(pos)): #para cada item\n","            data.append( str(all_user.index(u)) + '\\t' + str(list(all_items).index(pos[i]) ) + '\\t' + str(neg[i]) )\n","            \n","    with open(filename, 'w')as fout:\n","        fout.write('\\n'.join(data))\n","\n","\n","\n","def AUC(rating, test_users,dict_tov):\n","  \"\"\"\n","  para cada usuario cuenta cuantos items de una muestra aleatoria tiene rating menor a \n","  al rating del item consumido en test\n","  \"\"\"\n","  ans=0\n","  cc=0\n","  for user in test_users:\n","      user_ind= test_users.index(user)\n","      user_= all_user[user]\n","      if dict_tov[user_][0] in all_items:\n","          item_test= list(all_items).index(dict_tov[user_][0])\n","          cc+=1\n","          items_train= [ list(all_items).index(i) for i in user_pos_train[user_]]  \n","          no_considerar= set(items_train+ [item_test]) \n","          count=0\n","          tmpans=0 \n","          for j in random.sample(range(ITEM_NUM),int(50*(len(no_considerar)-1))): #sample\n","              if j in no_considerar: continue\n","              if rating[user_ind,item_test]>rating[user_ind,j]: tmpans+=1\n","              count+=1\n","\n","          tmpans/=float(count)\n","          ans+=tmpans\n","  \n","  ans/=float(cc)\n","  return ans\n","\n","\n","\n","def simple_test(sess, model, dict_tov): \n","  #Calcula AUC para todos los usuarios comparada con el elemento que quedo en el test set\n","  #dict_tov: test o validacion\n"," test_users=[ all_user.index(i) for i in dict_tov.keys()]\n"," user_batch_rating = sess.run(model.all_rating, {model.u: test_users}) \n"," batch_result= AUC(user_batch_rating,test_users, dict_tov)\n","    \n"," return batch_result"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XHTKo43Jxfpd","scrolled":false},"source":["tf.compat.v1.reset_default_graph()\n","param = None\n","discriminator = DIS(ITEM_NUM, USER_NUM, EMB_DIM, visual_matrix, K2, lamda=0.001, param=param, initdelta=0.1, learning_rate=0.1, imageFeatureDim=1536) #se llama al discriminador DNS\n","\n","\n","config = tf.compat.v1.ConfigProto()\n","config.gpu_options.allow_growth = True\n","sess = tf.compat.v1.Session(config=config)\n","sess.run(tf.compat.v1.global_variables_initializer())\n","\n","generate_dns(sess, discriminator, DIS_TRAIN_FILE)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"background_save":true},"id":"LaqhR-XZ9Q7k","scrolled":false,"outputId":"bcb70602-92ae-444f-e481-8c22c1e494ff"},"source":["dis_log = open(workdir+'dis_log_dns_IRGANvisual.txt', 'w')\n","best_AUC=simple_test(sess, discriminator, user_pos_train)\n","#print(sess.run(discriminator.pre_loss,feed_dict={discriminator.u: [0], discriminator.pos: [1],discriminator.neg: [2]}))\n","#print(\"auc discriminador inicializado random\", best_AUC)\n","losses=[]\n","aucs=[]\n","for epoch in range(3): #80 antes\n","  loss_d_=0\n","  c=0\n","  generate_dns(sess, discriminator, DIS_TRAIN_FILE)  # dynamic negative sample tarda 20 seg\n","  with open(DIS_TRAIN_FILE)as fin:\n","\n","          for line in fin:\n","              line = line.split()\n","              c+=1\n","              u = int(line[0])\n","              i = int(line[1])\n","              j = int(line[2])\n","              #se actualiza el discriminador para la nueva tripleta\n","              _ ,loss= sess.run([discriminator.d_updates, discriminator.pre_loss ],\n","                             feed_dict={discriminator.u: [u], discriminator.pos: [i],\n","                                        discriminator.neg: [j]})\n","            \n","            \n","              #print(line)\n","              #print(loss, \"  :loss\")\n","              #print(sess.run([discriminator.reg, discriminator.pre_logits , discriminator.pre_loss],feed_dict={discriminator.u: [u], discriminator.pos: [i],discriminator.neg: [j]}), \"  :reg , prelogits\")\n","              loss_d_+=loss\n","                \n","  losses.append(loss_d_/c)\n","  AUC_actual = simple_test(sess, discriminator,user_pos_train) #evalua\n","  print (\"epoch \", epoch, \"dis: \", AUC_actual, \"loss: \",loss_d_/c )\n","  aucs.append(AUC_actual)\n","    \n","  if AUC_actual> best_AUC:\n","          print(\"mejore\")\n","          best_AUC = AUC_actual\n","          discriminator.save_model(sess, DIS_MODEL_FILE)\n","  \n","  print (\"best AUC: \", best_AUC)\n","\n","  buf = '\\t'.join([str(AUC_actual)])\n","  dis_log.write(str(epoch) + '\\t' + buf + '\\n')\n","  dis_log.flush()\n","\n","dis_log.close()"],"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["epoch  0 dis:  0.8900728626147765 loss:  0.9419832744527792\n","mejore\n","best AUC:  0.8900728626147765\n","epoch  1 dis:  0.9104114638250504 loss:  0.8962275549279018\n","mejore\n","best AUC:  0.9104114638250504\n"]}]},{"cell_type":"code","metadata":{"id":"7IkTKkqm4Oow"},"source":["losses"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-Z13WUMp4SzP"},"source":["aucs"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZdC_lp1Z12Bw"},"source":[""]},{"cell_type":"markdown","metadata":{"id":"rnRscRVjxfph"},"source":["## minmax"]},{"cell_type":"markdown","metadata":{"id":"hf2mwD7WEMQT"},"source":["la regularizacion es crucial"]},{"cell_type":"code","metadata":{"id":"fALaOQpbuvA3","colab":{"base_uri":"https://localhost:8080/"},"outputId":"38df772e-fb53-4f0b-e194-63ae8bdaeb9d"},"source":["BATCH_SIZE = 1000 #16\n","import pickle\n","print (\"load model...\")\n","\n","\n","with open( workdir+ \"model_dns_IRGANvisual.pkl\", 'rb') as f: \n","  param = pickle.load(f,encoding=\"bytes\")\n","\n","generator = GEN(ITEM_NUM, USER_NUM, EMB_DIM, visual_matrix, K2,lamda=0.001, param=param, initdelta=0.1,\n","                    learning_rate=0.01, imageFeatureDim=1536 )\n","\n","discriminator = DIS(ITEM_NUM, USER_NUM, EMB_DIM,visual_matrix, K2, lamda=0.001, param=None, initdelta=0.1,\n","                        learning_rate=0.01, imageFeatureDim=1536) #segundo discriminador (el que no es DNS)\n","\n","config = tf.compat.v1.ConfigProto()\n","config.gpu_options.allow_growth = True\n","sess = tf.compat.v1.Session(config=config)\n","sess.run(tf.compat.v1.global_variables_initializer())\n","g1= simple_test(sess, generator,user_pos_train)\n","d1=simple_test(sess, discriminator,user_pos_train)\n","print (\"gen \", g1)\n","print (\"dis \", d1)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["load model...\n","gen  0.9116949460266968\n","dis  0.49850717191236965\n"]}]},{"cell_type":"markdown","metadata":{"id":"lYs2IPRUH15o"},"source":["load model...\n","gen  0.9116949460266968\n","dis  0.49850717191236965"]},{"cell_type":"code","metadata":{"id":"nrupOJrIwSze"},"source":["\n","def generate_for_d(sess, model, filename):\n","    data = []\n","    for u in user_pos_train: #para cada usario\n","        pos = user_pos_train[u] #sus items \n","        rating = sess.run(model.all_rating, {model.u: [all_user.index(u)]}) #completar la matriz \n","        rating = np.array(rating[0]) #/ 0.2  # Temperature #infla los rating #plotear que pasa con la sigmoide cuando ponemos temperatura\n","        exp_rating = np.exp(rating) \n","        prob = exp_rating / np.sum(exp_rating)\n","        #print(u)\n","        neg = np.random.choice(list(all_items), size=len(pos), p=prob) #muestreo negativo \n","        for i in range(len(pos)):\n","            data.append( str(all_user.index(u)) + '\\t' + str(list(all_items).index(pos[i]) ) + '\\t' + str(list(all_items).index(neg[i])) ) \n","\n","    with open(filename, 'w')as fout:\n","        fout.write('\\n'.join(data))\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lQemBB4FvZwS","scrolled":false,"outputId":"ced45d1c-9a12-41ad-a13c-381fcb13b81e"},"source":["# minimax training\n","dis_log = open('dis_log.txt', 'w')\n","gen_log = open('gen_log.txt', 'w')\n","loss_d=[]\n","auc_epocs_g=[]\n","auc_epocs_d=[]\n","loss_g=[]\n","for epoch in range(15):#15\n","    print(\"inicia minmax\")\n","    if epoch >= 0:\n","        #entrena D \n","        for d_epoch in range(10):##100 #para el generador fijo, entrena al discriminador\n","           # print(\"empieza a entrenarse el discriminador\")\n","            loss_d_=0\n","            if d_epoch % 5 == 0:\n","                generate_for_d(sess, generator, DIS_TRAIN_FILE) #crea un archivo con las tripletas\n","                #print(\"pase\")\n","                train_size = file_len(DIS_TRAIN_FILE)\n","            index = 1\n","            c=0\n","            while index < train_size-BATCH_SIZE: #hacer minibatch con etiquetado si es ejemplo positivo o negativo\n","                if index + BATCH_SIZE <= train_size + 1:\n","                    input_user, input_item, input_label = get_batch_data(DIS_TRAIN_FILE, index, BATCH_SIZE)\n","                else:\n","                    input_user, input_item, input_label = get_batch_data(DIS_TRAIN_FILE, index,\n","                                                                            train_size - index + 1)\n","                index += BATCH_SIZE\n","               # print(index)\n","                _, loss_dis = sess.run([discriminator.d_updates, discriminator.pre_loss],\n","                                 feed_dict={discriminator.u: input_user, discriminator.i: input_item, discriminator.label: input_label}) #actualizaciones de los embeddings y bias tales que clasifiquen bien\n","                loss_d_+=loss_dis\n","                c+=1\n","            \n","            loss_d.append(loss_d_/c)\n","        \n","            auc_actual = simple_test(sess, discriminator, user_pos_train)\n","            auc_epocs_d.append(auc_actual)\n","            #print (\"epoch \", epoch, \"dis: \", auc_actual)\n","            if auc_actual > best_AUC:\n","                print ('mejore: ', auc_actual)\n","                best_AUC = auc_actual\n","\n","        for g_epoch in range(5):  # 50 #para el discriminador fijo (con pesos actualizados), se entrena el generador\n","            #print(\"empieza a entrenarse el generador\")\n","            c=0\n","            loss_g_=0\n","            for u in user_pos_train: #por cada usuario\n","                sample_lambda = 0.2 #temperatura\n","                pos = [list(all_items).index(i) for i in user_pos_train[u]] #peliculas relevantes\n","                rating = sess.run(generator.all_logits, {generator.u: all_user.index(u)}) #calcula los ratings para ese usuario\n","                exp_rating = np.exp(rating)\n","                prob = exp_rating / np.sum(exp_rating)  # prob is generator distribution p_\\theta\n","                pn = (1 - sample_lambda) * prob\n","                pn[pos] += sample_lambda * 1.0 / len(pos) #pedirle las prob a lo consumido + 0.2/la cantidad de items consumidos\n","                # Now, pn is the Pn in importance sampling, prob is generator distribution p_\\theta\n","                sample = np.random.choice(np.arange(ITEM_NUM), 2 * len(pos), p=pn) #elige el doble de items c/r a lo consumido\n","                \n","                #               # Get reward and adapt it with importance sampling                #######\n","                reward = sess.run(discriminator.reward, {discriminator.u: all_user.index(u), discriminator.i: sample})\n","                reward = reward * prob[sample] / pn[sample]\n","                ######             # Update G              ####################\n","                _, loss_gen = sess.run([generator.gan_updates, generator.gan_loss],{generator.u: all_user.index(u), generator.i: sample, generator.reward: reward}) #actualiza pesos\n","                c+=1\n","                loss_g_+=loss_gen\n","                \n","            loss_g.append(loss_g_/c)\n","            auc_actual = simple_test(sess, generator, user_pos_train)\n","            auc_epocs_g.append(auc_actual)\n","            print (\"epoch \", epoch, \"gen: \", auc_actual)\n","            buf = '\\t'.join([str(auc_actual) ])\n","            gen_log.write(str(epoch) + '\\t' + buf + '\\n')\n","            gen_log.flush()\n","            if auc_actual > best_AUC:\n","                print ('mejore: ', auc_actual)\n","                best_AUC = auc_actual\n","                generator.save_model(sess, \"gan_generator.pkl\") #se guarda el mejor modelo, la mejor factorización matricial.\n","\n","gen_log.close()\n","dis_log.close()\n","\n"],"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["inicia minmax\n","epoch  0 gen:  0.8959414851080767\n","mejore:  0.8959414851080767\n","epoch  0 gen:  0.8995279548939695\n","mejore:  0.8995279548939695\n","epoch  0 gen:  0.8976934032393626\n","epoch  0 gen:  0.8969843891797786\n","epoch  0 gen:  0.8946711502769191\n","inicia minmax\n","epoch  1 gen:  0.8904483319985119\n","epoch  1 gen:  0.8875710015029633\n","epoch  1 gen:  0.8853620669252406\n","epoch  1 gen:  0.8822660399973065\n","epoch  1 gen:  0.8782526398068847\n","inicia minmax\n","epoch  2 gen:  0.8752067377718498\n","epoch  2 gen:  0.8697559470309231\n","epoch  2 gen:  0.8666775232741865\n","epoch  2 gen:  0.8628582728018616\n","epoch  2 gen:  0.8590917068029897\n","inicia minmax\n","epoch  3 gen:  0.8533357582707628\n","epoch  3 gen:  0.850087628490991\n","epoch  3 gen:  0.8457106726255152\n","epoch  3 gen:  0.8410731689149065\n","epoch  3 gen:  0.8369502689989318\n","inicia minmax\n","epoch  4 gen:  0.8326232939310749\n","epoch  4 gen:  0.8266149859475033\n","epoch  4 gen:  0.8229670725544892\n","epoch  4 gen:  0.8176627799442207\n","epoch  4 gen:  0.8128138728168106\n","inicia minmax\n","epoch  5 gen:  0.8076066132699463\n","epoch  5 gen:  0.8033201409837393\n","epoch  5 gen:  0.7984004739105329\n","epoch  5 gen:  0.7957470469823558\n","epoch  5 gen:  0.7896180317558363\n","inicia minmax\n","epoch  6 gen:  0.7841366543054312\n","epoch  6 gen:  0.7799284465742976\n","epoch  6 gen:  0.776136315508038\n","epoch  6 gen:  0.7700177847882702\n","epoch  6 gen:  0.7680321928711086\n","inicia minmax\n","epoch  7 gen:  0.7634514993594521\n","epoch  7 gen:  0.7587127824490878\n","epoch  7 gen:  0.7543038139343927\n","epoch  7 gen:  0.7511580720804876\n","epoch  7 gen:  0.746912676873635\n","inicia minmax\n","epoch  8 gen:  0.7429607413013936\n","epoch  8 gen:  0.7395537249235893\n","epoch  8 gen:  0.7365598667583019\n"]}]},{"cell_type":"code","metadata":{"id":"OOdJ5Eje4xlO","colab":{"base_uri":"https://localhost:8080/","height":241},"outputId":"ea215210-77be-48ea-e222-de7be233aab9"},"source":["print(simple_test(sess, discriminator,user_pos_train))\n","print(simple_test(sess, generator,user_pos_train))\n","print(simple_test(sess, discriminator,user_pos_test))\n","print(simple_test(sess, generator,user_pos_test))\n","\n","\n","textfile = open(workdir+\"IRGAN_visual.txt\", \"w\")\n","textfile.write(str(loss_d) + \"\\n\")\n","textfile.write(str(auc_epocs_g) + \"\\n\")\n","textfile.write(str(auc_epocs_d) + \"\\n\")\n","textfile.write(str(loss_g) + \"\\n\")\n","textfile.close()"],"execution_count":null,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-15-1cfc450fd68e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msimple_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdiscriminator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0muser_pos_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msimple_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0muser_pos_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msimple_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdiscriminator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0muser_pos_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msimple_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0muser_pos_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'sess' is not defined"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"75HJHjNEfLjU","outputId":"a4eb8daf-6caf-4b7c-dcf3-73010a422d7f"},"source":["with open( workdir+ \"gan_generator.pkl\", 'rb') as f: \n","  param = pickle.load(f,encoding=\"bytes\")\n","\n","generator = GEN(ITEM_NUM, USER_NUM, EMB_DIM, visual_matrix, K2,lamda=0.001, param=param, initdelta=0.1,\n","                    learning_rate=0.01, imageFeatureDim=1536 )\n","textfile = open(workdir+\"IRGAN_visual.txt\", \"w\")\n","config = tf.compat.v1.ConfigProto()\n","config.gpu_options.allow_growth = True\n","sess = tf.compat.v1.Session(config=config)\n","sess.run(tf.compat.v1.global_variables_initializer())\n","\n","\n","print(simple_test(sess, generator,user_pos_train))\n","print(simple_test(sess, generator,user_pos_test))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["0.8918364826814386\n","0.7962727415096519\n"]}]},{"cell_type":"markdown","metadata":{"id":"IqvinsXfruUV"},"source":[""]},{"cell_type":"markdown","metadata":{"id":"PaK8Nx1oruxQ"},"source":["# boots"]},{"cell_type":"markdown","metadata":{"id":"JZKwLkFuruxR"},"source":["## Inicialiazacion DNS"]},{"cell_type":"code","metadata":{"id":"JYotQYzSsBY5"},"source":["DIS_TRAIN_FILE =workdir+ \"dis-train_boots.txt\"\n","DIS_MODEL_FILE =   workdir+\"model_dns_IRGANvisual_boots.pkl\" #rutaaaa"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"M3TRE2d7ruxR"},"source":["\n","\n","\n","def AUC(rating, test_users,dict_tov):\n","  \"\"\"\n","  para cada usuario cuenta cuantos items de una muestra aleatoria tiene rating menor a \n","  al rating del item consumido en test\n","  \"\"\"\n","  ans=0\n","  cc=0\n","  for user in test_users:\n","      user_ind= test_users.index(user)\n","      user_= all_user[user]\n","      if dict_tov[user_][0] in all_items:\n","          item_test= list(all_items).index(dict_tov[user_][0])\n","          cc+=1\n","          items_train= [ list(all_items).index(i) for i in user_pos_train[user_]]  \n","          no_considerar= set(items_train+ [item_test]) \n","          count=0\n","          tmpans=0 \n","          for j in random.sample(range(ITEM_NUM),int(50*(len(no_considerar)-1))): #sample\n","              if j in no_considerar: continue\n","              if rating[user_ind,item_test]>rating[user_ind,j]: tmpans+=1\n","              count+=1\n","\n","          tmpans/=float(count)\n","          ans+=tmpans\n","  \n","  ans/=float(cc)\n","  return ans\n","\n","\n","\n","def simple_test(sess, model, dict_tov): \n","  #Calcula AUC para todos los usuarios comparada con el elemento que quedo en el test set\n","  #dict_tov: test o validacion\n"," test_users=[ all_user.index(i) for i in dict_tov.keys()]\n"," user_batch_rating = sess.run(model.all_rating, {model.u: test_users}) \n"," batch_result= AUC(user_batch_rating,test_users, dict_tov)\n","    \n"," return batch_result"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"scrolled":false,"id":"5-AUf7FsruxR"},"source":["tf.compat.v1.reset_default_graph()\n","param = None\n","discriminator = DIS(ITEM_NUM, USER_NUM, EMB_DIM, visual_matrix, K2, lamda=0.001, param=param, initdelta=0.1, learning_rate=0.1, imageFeatureDim=1536) #se llama al discriminador DNS\n","\n","\n","config = tf.compat.v1.ConfigProto()\n","config.gpu_options.allow_growth = True\n","sess = tf.compat.v1.Session(config=config)\n","sess.run(tf.compat.v1.global_variables_initializer())\n","\n","#generate_dns(sess, discriminator, DIS_TRAIN_FILE)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bneC4z8l8BDA"},"source":["no sirve considerar todos ni tomar el top algo sobre una muestra mas grande"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"scrolled":false,"id":"8pajrsMCruxR","outputId":"497e9ea5-9a90-47a4-8b1c-e1f5e36e1659"},"source":["%%time\n","dis_log = open(workdir+'dis_log_dns_IRGANvisual.txt', 'w')\n","best_AUC=simple_test(sess, discriminator, user_pos_train)\n","#print(sess.run(discriminator.pre_loss,feed_dict={discriminator.u: [0], discriminator.pos: [1],discriminator.neg: [2]}))\n","#print(\"auc discriminador inicializado random\", best_AUC)\n","losses=[]\n","DNS_K= 5\n","aucs=[]\n","for epoch in range(int(13000)): \n","        u= np.random.choice(list(user_pos_train.keys()),1)[0]\n","        i = list(all_items).index(np.random.choice(user_pos_train[u],1)[0] ) #lo que consumio\n","        pos = user_pos_train[u]\n","        all_rating = sess.run(discriminator.dns_rating, {discriminator.u: all_user.index(u)}) #evalua el modelo\n","        all_rating = np.array(all_rating)\n","        neg = []\n","        candidates = list(set(all_items) - set(pos) )#los que no ha consumido\n","        \n","        choice = np.random.choice(candidates, DNS_K) #se escogen DN_K aleatoriamente de los candidatos\n","        choice= [list(all_items).index(i) for i in choice]\n","        j= np.random.choice(choice,1)[0]\n","        u= all_user.index(u)\n","\n","              #se actualiza el discriminador para la nueva tripleta\n","        _ ,loss= sess.run([discriminator.d_updates, discriminator.pre_loss ],\n","                             feed_dict={discriminator.u: [u], discriminator.pos: [i],\n","                                        discriminator.neg: [j]})\n","                \n","        if epoch%1000==0:\n","          losses.append(loss)\n","          AUC_actual = simple_test(sess, discriminator,user_pos_train) #evalua\n","          print (\"epoch \", epoch, \"dis: \", AUC_actual, \"loss: \",loss )\n","          aucs.append(AUC_actual)\n","            \n","          if AUC_actual> best_AUC:\n","                  print(\"mejore\")\n","                  best_AUC = AUC_actual\n","                  discriminator.save_model(sess, DIS_MODEL_FILE)\n","                  print (\"best AUC: \", best_AUC)\n","                  \n","        buf = '\\t'.join([str(AUC_actual)])\n","        dis_log.write(str(epoch) + '\\t' + buf + '\\n')\n","        dis_log.flush()\n","\n","dis_log.close()"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["epoch  0 dis:  0.5012954302627196 loss:  2.0601177\n","mejore\n","best AUC:  0.5012954302627196\n","epoch  1000 dis:  0.6149333506607484 loss:  1.1278698\n","mejore\n","best AUC:  0.6149333506607484\n","epoch  2000 dis:  0.657589176761334 loss:  1.404794\n","mejore\n","best AUC:  0.657589176761334\n","epoch  3000 dis:  0.6896867464457391 loss:  1.199521\n","mejore\n","best AUC:  0.6896867464457391\n","epoch  4000 dis:  0.7054822704227821 loss:  1.2387245\n","mejore\n","best AUC:  0.7054822704227821\n","epoch  5000 dis:  0.722411302996846 loss:  4.0489035\n","mejore\n","best AUC:  0.722411302996846\n","epoch  6000 dis:  0.742912918926421 loss:  0.6096073\n","mejore\n","best AUC:  0.742912918926421\n","epoch  7000 dis:  0.7599264363059726 loss:  0.45701966\n","mejore\n","best AUC:  0.7599264363059726\n","epoch  8000 dis:  0.7676438299688787 loss:  0.44468033\n","mejore\n","best AUC:  0.7676438299688787\n","epoch  9000 dis:  0.7844978125646576 loss:  0.44688514\n","mejore\n","best AUC:  0.7844978125646576\n","epoch  10000 dis:  0.7936023980644149 loss:  0.34763336\n","mejore\n","best AUC:  0.7936023980644149\n","epoch  11000 dis:  0.8049058094112108 loss:  0.5445442\n","mejore\n","best AUC:  0.8049058094112108\n","epoch  12000 dis:  0.8162501266976148 loss:  4.2339935\n","mejore\n","best AUC:  0.8162501266976148\n","CPU times: user 5min 20s, sys: 19.5 s, total: 5min 39s\n","Wall time: 5min 38s\n"]}]},{"cell_type":"markdown","metadata":{"id":"Ry3aQqjZ1aDa"},"source":["10 min"]},{"cell_type":"code","metadata":{"id":"iPHNS7r3ruxR","colab":{"base_uri":"https://localhost:8080/"},"outputId":"b897c692-7f61-442e-9a91-03ab98c41c1f"},"source":["losses"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[2.0601177,\n"," 1.1278698,\n"," 1.404794,\n"," 1.199521,\n"," 1.2387245,\n"," 4.0489035,\n"," 0.6096073,\n"," 0.45701966,\n"," 0.44468033,\n"," 0.44688514,\n"," 0.34763336,\n"," 0.5445442,\n"," 4.2339935]"]},"metadata":{},"execution_count":17}]},{"cell_type":"code","metadata":{"id":"pimExmA0ruxR","colab":{"base_uri":"https://localhost:8080/"},"outputId":"6f24decb-a3f2-41c5-a53d-f788ff5ab2a7"},"source":["aucs"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[0.5012954302627196,\n"," 0.6149333506607484,\n"," 0.657589176761334,\n"," 0.6896867464457391,\n"," 0.7054822704227821,\n"," 0.722411302996846,\n"," 0.742912918926421,\n"," 0.7599264363059726,\n"," 0.7676438299688787,\n"," 0.7844978125646576,\n"," 0.7936023980644149,\n"," 0.8049058094112108,\n"," 0.8162501266976148]"]},"metadata":{},"execution_count":18}]},{"cell_type":"code","metadata":{"id":"b9prPNZB76do","outputId":"6a705f70-c4c3-4a77-c8a4-a2cb56a98c9a","colab":{"base_uri":"https://localhost:8080/"}},"source":["print(simple_test(sess, discriminator,user_pos_train))\n","print(simple_test(sess, discriminator,user_pos_test))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["0.82422036442449\n","0.8150057882059594\n"]}]},{"cell_type":"markdown","metadata":{"id":"AAGIbMDgruxR"},"source":["## minmax"]},{"cell_type":"markdown","metadata":{"id":"1DVYZKcxruxR"},"source":["la regularizacion es crucial"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ylCrkCWDruxR","outputId":"5c2c2353-bffa-4a2c-9128-f882c5cb7622"},"source":["BATCH_SIZE = 1000 #16\n","import pickle\n","print (\"load model...\")\n","\n","\n","with open( workdir+ \"model_dns_IRGANvisual_boots.pkl\", 'rb') as f: \n","  param = pickle.load(f,encoding=\"bytes\")\n","\n","generator = GEN(ITEM_NUM, USER_NUM, EMB_DIM, visual_matrix, K2,lamda=0.001, param=param, initdelta=0.1,\n","                    learning_rate=0.01, imageFeatureDim=1536 )\n","\n","discriminator = DIS(ITEM_NUM, USER_NUM, EMB_DIM,visual_matrix, K2, lamda=0.001, param=None, initdelta=0.1,\n","                        learning_rate=0.01, imageFeatureDim=1536) #segundo discriminador (el que no es DNS)\n","\n","config = tf.compat.v1.ConfigProto()\n","config.gpu_options.allow_growth = True\n","sess = tf.compat.v1.Session(config=config)\n","sess.run(tf.compat.v1.global_variables_initializer())\n","g1= simple_test(sess, generator,user_pos_train)\n","d1=simple_test(sess, discriminator,user_pos_train)\n","print (\"gen \", g1)\n","print (\"dis \", d1)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["load model...\n","gen  0.8164278496177005\n","dis  0.5022067687503954\n"]}]},{"cell_type":"code","metadata":{"id":"6zqybatpBRW9","colab":{"base_uri":"https://localhost:8080/"},"outputId":"99484955-bd17-4d12-d506-1c4ab0ec6caa"},"source":["simple_test(sess, generator,user_pos_test)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.8020417637172242"]},"metadata":{},"execution_count":23}]},{"cell_type":"code","metadata":{"id":"3ChEWNCr821-","outputId":"3943025f-aa74-40f7-e068-dc401a7113ec","colab":{"base_uri":"https://localhost:8080/"}},"source":["print(simple_test(sess, generator,user_pos_train))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["0.8153143939135364\n"]}]},{"cell_type":"code","metadata":{"id":"KC6VhlmlruxR"},"source":["\n","def generate_for_d(sess, model, filename):\n","    data = []\n","    for u in user_pos_train: #para cada usario\n","        pos = user_pos_train[u] #sus items \n","        rating = sess.run(model.all_rating, {model.u: [all_user.index(u)]}) #completar la matriz \n","        rating = np.array(rating[0]) #/ 0.2  # Temperature #infla los rating #plotear que pasa con la sigmoide cuando ponemos temperatura\n","        exp_rating = np.exp(rating) \n","        prob = exp_rating / np.sum(exp_rating)\n","        #print(u)\n","        neg = np.random.choice(list(all_items), size=len(pos), p=prob) #muestreo negativo \n","        for i in range(len(pos)):\n","            data.append( str(all_user.index(u)) + '\\t' + str(list(all_items).index(pos[i]) ) + '\\t' + str(list(all_items).index(neg[i])) ) \n","\n","    with open(filename, 'w')as fout:\n","        fout.write('\\n'.join(data))\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"scrolled":false,"id":"e-JN8iRGruxR","outputId":"ec941f5f-7e8a-4862-d6ef-fe143c28f7d1"},"source":["# minimax training\n","dis_log = open('dis_log_boots.txt', 'w')\n","gen_log = open('gen_log_boots.txt', 'w')\n","loss_d=[]\n","auc_epocs_g=[]\n","auc_epocs_d=[]\n","loss_g=[]\n","\n","for epoch in range(15):#15\n","    print(\"inicia minmax\")\n","    if epoch >= 0:\n","        #entrena D \n","         for d_epoch in range(1):##100 #para el generador fijo, entrena al discriminador\n","           # print(\"empieza a entrenarse el discriminador\")\n","            loss_d_=0\n","            if d_epoch % 5 == 0:\n","                generate_for_d(sess, generator, DIS_TRAIN_FILE) #crea un archivo con las tripletas\n","                #print(\"pase\")\n","                train_size = file_len(DIS_TRAIN_FILE)\n","            index = 1\n","            c=0\n","            while index < train_size-BATCH_SIZE: #hacer minibatch con etiquetado si es ejemplo positivo o negativo\n","                if index + BATCH_SIZE <= train_size + 1:\n","                    input_user, input_item, input_label = get_batch_data(DIS_TRAIN_FILE, index, BATCH_SIZE)\n","                else:\n","                    input_user, input_item, input_label = get_batch_data(DIS_TRAIN_FILE, index,\n","                                                                            train_size - index + 1)\n","                index += BATCH_SIZE\n","               # print(index)\n","                _, loss_dis = sess.run([discriminator.d_updates, discriminator.pre_loss],\n","                                 feed_dict={discriminator.u: input_user, discriminator.i: input_item, discriminator.label: input_label}) #actualizaciones de los embeddings y bias tales que clasifiquen bien\n","                loss_d_+=loss_dis\n","                c+=1\n","            \n","            loss_d.append(loss_d_/c)\n","            auc_actual = simple_test(sess, discriminator, user_pos_train)\n","            auc_epocs_d.append(auc_actual)\n","            print (\"epoch \", epoch, \"dis: \", loss_d_/c)\n","            if auc_actual > best_AUC:\n","                print ('mejore: ', auc_actual)\n","                best_AUC = auc_actual\n","\n","         for g_epoch in range(1):  #\n","            #print(\"empieza a entrenarse el generador\")\n","            c=0\n","            loss_g_=0\n","            for u in user_pos_train: #por cada usuario #por cada usuario\n","                #u= np.random.choice(list(user_pos_train.keys()),1)[0]\n","                sample_lambda = 0.2 #temperatura\n","                pos = [list(all_items).index(i) for i in user_pos_train[u]] #peliculas relevantes\n","                rating = sess.run(generator.all_logits, {generator.u: all_user.index(u)}) #calcula los ratings para ese usuario\n","                exp_rating = np.exp(rating)\n","                prob = exp_rating / np.sum(exp_rating)  # prob is generator distribution p_\\theta\n","                pn = (1 - sample_lambda) * prob\n","                pn[pos] += sample_lambda * 1.0 / len(pos) #pedirle las prob a lo consumido + 0.2/la cantidad de items consumidos\n","                # Now, pn is the Pn in importance sampling, prob is generator distribution p_\\theta\n","                sample = np.random.choice(np.arange(ITEM_NUM), 2 * len(pos), p=pn) #elige el doble de items c/r a lo consumido\n","                \n","                #               # Get reward and adapt it with importance sampling                #######\n","                reward = sess.run(discriminator.reward, {discriminator.u: all_user.index(u), discriminator.i: sample})\n","                reward = reward * prob[sample] / pn[sample]\n","                ######             # Update G              ####################\n","                _, loss_gen = sess.run([generator.gan_updates, generator.gan_loss],{generator.u: all_user.index(u), generator.i: sample, generator.reward: reward}) #actualiza pesos\n","                c+=1\n","                loss_g_+=loss_gen\n","                \n","            loss_g.append(loss_g_/c)\n","            auc_actual = simple_test(sess, generator, user_pos_train)\n","            auc_epocs_g.append(auc_actual)\n","            print (\"epoch \", epoch, \"gen: \", auc_actual)\n","            buf = '\\t'.join([str(auc_actual) ])\n","            gen_log.write(str(epoch) + '\\t' + buf + '\\n')\n","            gen_log.flush()\n","            if auc_actual > best_AUC:\n","                print ('mejore: ', auc_actual)\n","                best_AUC = auc_actual\n","                generator.save_model(sess, \"gan_generator_boots.pkl\") #se guarda el mejor modelo, la mejor factorización matricial.\n","\n","gen_log.close()\n","dis_log.close()\n","\n"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["inicia minmax\n","epoch  0 dis:  [7.7057953 7.6309066 7.7027345 ... 7.6953998 7.641296  7.771368 ]\n","epoch  0 gen:  0.8155650781712475\n","inicia minmax\n","epoch  1 dis:  [5.669022  5.6958737 5.637288  ... 5.658829  5.748808  5.69489  ]\n","epoch  1 gen:  0.8138619305945937\n","inicia minmax\n","epoch  2 dis:  [7.30206   6.631453  6.5998197 ... 5.242341  5.643454  5.1726227]\n","epoch  2 gen:  0.8120950229228414\n","inicia minmax\n","epoch  3 dis:  [594.89355   83.14791   15.764571 ...  25.180834 113.66809   26.986382]\n","epoch  3 gen:  0.8099995749257318\n","inicia minmax\n","epoch  4 dis:  [8378.723  6530.4175 2474.9802 ... 1941.1824 1125.6984 2615.1045]\n","epoch  4 gen:  0.8080118800411334\n","inicia minmax\n","epoch  5 dis:  [ 14860.909 227441.98   20191.14  ...  33369.184  73123.195  44738.406]\n","epoch  5 gen:  0.8068472202491901\n","inicia minmax\n","epoch  6 dis:  [2438697.  2209621.5 5404261.  ... 1478476.  1629228.  1358188.2]\n","epoch  6 gen:  0.8043031639295845\n","inicia minmax\n","epoch  7 dis:  [2.9203386e+07 7.9842893e+08 3.3795436e+07 ... 4.2170048e+07 3.0952794e+07\n"," 3.4845716e+07]\n","epoch  7 gen:  0.8038223090513105\n","inicia minmax\n","epoch  8 dis:  [6.7014246e+09 1.4066410e+10 9.2791255e+09 ... 5.1393695e+09 1.5485844e+09\n"," 4.7139615e+09]\n","epoch  8 gen:  0.8013131159184395\n","inicia minmax\n","epoch  9 dis:  [1.1743616e+12 3.1775415e+11 3.6400506e+11 ... 7.1991599e+10 2.3297145e+11\n"," 6.3234195e+10]\n","epoch  9 gen:  0.8005090612222256\n","inicia minmax\n","epoch  10 dis:  [3.2048075e+13 4.6935323e+13 9.7077795e+12 ... 7.1937399e+12 4.7786943e+12\n"," 7.1666730e+12]\n","epoch  10 gen:  0.7965501302212163\n","inicia minmax\n","epoch  11 dis:  [3.44381242e+15 1.82851589e+15 1.03547924e+15 ... 3.43202750e+14\n"," 6.32046314e+14 4.17522663e+14]\n","epoch  11 gen:  0.7945853251168059\n","inicia minmax\n","epoch  12 dis:  [1.4618930e+17 1.6989775e+16 5.6620657e+16 ... 2.3094969e+16 2.1412933e+16\n"," 3.0414538e+16]\n","epoch  12 gen:  0.7929576972491612\n","inicia minmax\n","epoch  13 dis:  [2.3818124e+18 1.3373655e+19 3.0801392e+17 ... 1.5007772e+18 2.0660985e+18\n"," 1.3551609e+18]\n","epoch  13 gen:  0.7907726613584223\n","inicia minmax\n","epoch  14 dis:  [1.5504989e+19 7.9946395e+20 3.1405685e+19 ... 2.8297629e+19 9.8127059e+19\n"," 2.4513629e+19]\n","epoch  14 gen:  0.7897047444965104\n"]}]},{"cell_type":"markdown","metadata":{"id":"qGBoRTVWLN0X"},"source":["20 min"]},{"cell_type":"code","metadata":{"id":"enjoDdZ_HdGt","outputId":"18ce2b44-498a-4804-9b73-55d933c2ac08","colab":{"base_uri":"https://localhost:8080/"}},"source":["print(simple_test(sess, discriminator,user_pos_train))\n","print(simple_test(sess, generator,user_pos_train))\n","print(simple_test(sess, discriminator,user_pos_test))\n","print(simple_test(sess, generator,user_pos_test))\n"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["0.4958307067582737\n","0.7889865269004773\n","0.5037783039593355\n","0.8062087304815215\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":245},"id":"EezvosxrruxR","outputId":"28527444-47ad-4165-879b-66b1b53cd74a"},"source":["with open( \"gan_generator_boots.pkl\", 'rb') as f: \n","  param = pickle.load(f,encoding=\"bytes\")\n","\n","generator = GEN(ITEM_NUM, USER_NUM, EMB_DIM, visual_matrix, K2,lamda=0.001, param=param, initdelta=0.1,\n","                    learning_rate=0.01, imageFeatureDim=1536 )\n","textfile = open(workdir+\"IRGAN_visual_boots.txt\", \"w\")\n","config = tf.compat.v1.ConfigProto()\n","config.gpu_options.allow_growth = True\n","sess = tf.compat.v1.Session(config=config)\n","sess.run(tf.compat.v1.global_variables_initializer())\n","\n","\n","print(simple_test(sess, generator,user_pos_train))\n","print(simple_test(sess, generator,user_pos_test))"],"execution_count":null,"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-29-9f70592630ec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0;34m\"gan_generator_boots.pkl\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m   \u001b[0mparam\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"bytes\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m generator = GEN(ITEM_NUM, USER_NUM, EMB_DIM, visual_matrix, K2,lamda=0.001, param=param, initdelta=0.1,\n\u001b[1;32m      5\u001b[0m                     learning_rate=0.01, imageFeatureDim=1536 )\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'gan_generator_boots.pkl'"]}]},{"cell_type":"markdown","metadata":{"id":"fnqc16npKAkl"},"source":["0.9198166097131708\n","\n","0.8090884325775983"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"b11j7413Y5vx","outputId":"e5659ab9-940f-4705-c531-f27772572030"},"source":["auc_epocs_g"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[0.7800054441783106,\n"," 0.778972125143461,\n"," 0.7759656284956431,\n"," 0.7743019204271282,\n"," 0.7725090918823623,\n"," 0.7705910719605862,\n"," 0.7680430120701902,\n"," 0.7647846357169482,\n"," 0.7622624390203705,\n"," 0.7607039350521542,\n"," 0.7576851938300642,\n"," 0.7555315762648882,\n"," 0.7535409105374787,\n"," 0.7492700907608366,\n"," 0.7479142114088607]"]},"metadata":{},"execution_count":49}]},{"cell_type":"code","metadata":{"id":"ViyDfhxNJ4JK","colab":{"base_uri":"https://localhost:8080/"},"outputId":"0ffbe4e2-2584-494d-f966-28f9dbb9ba28"},"source":["[np.mean(i) for i in loss_g]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[0.28027836768560005,\n"," 0.25151376621715094,\n"," 0.23450459214777578,\n"," 0.2208106676480953,\n"," 0.20411755238651422,\n"," 0.19206104918451564,\n"," 0.17876111284815588,\n"," 0.1735719848496098,\n"," 0.16736188621090672,\n"," 0.15523187488698376,\n"," 0.1464639587204439,\n"," 0.13973303543317267,\n"," 0.13236387254039664,\n"," 0.12362026906901184,\n"," 0.12183147811184622]"]},"metadata":{},"execution_count":50}]}]}